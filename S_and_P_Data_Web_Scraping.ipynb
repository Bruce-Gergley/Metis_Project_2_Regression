{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Python code I used for Metis Data Science Bootcamp Project 2\n",
    "# Some comments have been added and some unnecessary code blocks were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block imports Python libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block adjusts DataFrame display settings\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the '.get' functionality from the requests library to access the target website\n",
    "# A response status code of '200' indicates a successful API request\n",
    "\n",
    "S_and_P_500_URL = 'https://www.slickcharts.com/sp500'\n",
    "response = requests.get(S_and_P_500_URL)\n",
    "#response.status_code # To view the response status, remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the web-scraping functionality BeautifulSoup (from the bs4 library)\n",
    "# This code pulls the HTML from the website and accessed in the previous block and saves it as a bs4 object\n",
    "\n",
    "S_and_P_page = response.text\n",
    "S_and_P_soup = BeautifulSoup(S_and_P_page)\n",
    "#S_and_P_soup # To view the saved bs4 object, remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a blank Pandas DataFrame for the Name, Symbol, and Weight of each company\n",
    "# This DataFrame will be populated by a row of data for each company listed in the S&P 500\n",
    "\n",
    "S_and_P_DF = pd.DataFrame(columns = ('Company', 'Symbol', 'Weight'))\n",
    "#display(S_and_P_DF) # To view blank DataFrame, remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the '.find_all' functionality from bs4 to make a list using the table on the website\n",
    "# Each row from the table (denoted by 'tr' in the HTML) will be one item in the List_of_Rows\n",
    "# The first row (index number 0 on the list) is the header row with column names\n",
    "# Each subsequent row had the data for one S & P company listing\n",
    "\n",
    "List_of_Rows = list(S_and_P_soup.find_all('tr'))\n",
    "\n",
    "#List_of_Rows # To view the list of rows, remove the left most hashtag from this line\n",
    "#List_of_Rows[1] # To view a single example item from this list, remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a list with the most recent closing prices \n",
    "# Note: closing prices will differ from those used in my analysis, since you are running this on a different date\n",
    "\n",
    "List_of_Closings = []\n",
    "\n",
    "Index = 1 # Index initialized at 1 since index 0 refers to the header row, company data starts at 1\n",
    "\n",
    "while(Index < len(List_of_Rows)):\n",
    "    \n",
    "    Row = str(List_of_Rows[Index]) # Creates a string for a single company's data\n",
    "    \n",
    "    TD_List = list(Row.split('/td')) # Each column of data had a seperate <td></td> pair of tags\n",
    "    \n",
    "    Price_String = TD_List[4] # The Closing Price is in the 5th column (index number 4 on the TD_List)\n",
    "    #print(Price_String)\n",
    "    \n",
    "    Sub_Index = (len(Price_String)-1) # Price is near the end of the string with\n",
    "    \n",
    "    while(Price_String[Sub_Index] != \" \"): # These while loops find the indices needed to isolate the price data\n",
    "        Sub_Index -= 1\n",
    "    Sub_Index += 1\n",
    "    End_Index = Sub_Index\n",
    "    while(Price_String[End_Index] != \"<\"): \n",
    "        End_Index += 1\n",
    "    List_of_Closings.append((Price_String[Sub_Index:End_Index]).strip(\"\\xa0\"))\n",
    "    # The \"\\xa0\" characters are an artifact of converting from HTML to bs4 to test strings\n",
    "    \n",
    "    Index += 1\n",
    "    \n",
    "#List_of_Closings # To view the list of closing prices, remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates and populates lists for Company Name, S&P Symbol, and S&P Weight\n",
    "# These lists will be used to populate the corresp\n",
    "\n",
    "List_of_Companies = []\n",
    "List_of_Symbols = []\n",
    "List_of_Weights = []\n",
    "\n",
    "Index = 1\n",
    "\n",
    "while(Index < len(List_of_Rows)):\n",
    "    \n",
    "    Row = str(List_of_Rows[Index]) # Isolates a single company's data row\n",
    "    \n",
    "    # Initial Sub_Index value, and later incrementations, chosen to work for this specific site's HTML layout\n",
    "    Sub_Index = 13 \n",
    "    \n",
    "    # These Boolean variables track whether a data item for the current row has been added to its respective list\n",
    "    Company_Appended = False \n",
    "    Symbol_Appended = False\n",
    "    Weight_Appended = False\n",
    "    \n",
    "    while(Company_Appended == False): # Loop to append Company Name\n",
    "        \n",
    "        if(Row[Sub_Index:(Sub_Index+7)] == '<a href'):\n",
    "            Sub_Index += 18\n",
    "            while(Row[Sub_Index] != '>'):\n",
    "                Sub_Index += 1\n",
    "            Sub_Index += 1\n",
    "            Ending_Index = Sub_Index\n",
    "            while(Row[Ending_Index] != '<'):\n",
    "                Ending_Index += 1\n",
    "            List_of_Companies.append(str(Row[Sub_Index:Ending_Index]))\n",
    "            Company_Appended = True\n",
    "        else:\n",
    "            Sub_Index += 1\n",
    "            \n",
    "    Sub_Index += 13\n",
    "            \n",
    "    while(Symbol_Appended == False): # Loop to append Company Symbol\n",
    "        \n",
    "        if(Row[Sub_Index:(Sub_Index+7)] == '<a href'):\n",
    "            Sub_Index += 18\n",
    "            while(Row[Sub_Index] != '>'):\n",
    "                Sub_Index += 1\n",
    "            Sub_Index += 1\n",
    "            Ending_Index = Sub_Index\n",
    "            while(Row[Ending_Index] != '<'):\n",
    "                Ending_Index += 1\n",
    "            List_of_Symbols.append(str(Row[Sub_Index:Ending_Index]))\n",
    "            Symbol_Appended = True\n",
    "        else:\n",
    "            Sub_Index += 1\n",
    "            \n",
    "    Sub_Index += 9\n",
    "    \n",
    "    while(Weight_Appended == False): # Loop to append Company Weight\n",
    "        \n",
    "        if(Row[Sub_Index:(Sub_Index+4)] == '<td>'):\n",
    "            Sub_Index += 4\n",
    "            Ending_Index = Sub_Index\n",
    "            while(Row[Ending_Index] != '<'):\n",
    "                Ending_Index += 1\n",
    "            List_of_Weights.append(float(Row[Sub_Index:Ending_Index]))\n",
    "            Weight_Appended = True\n",
    "        else:\n",
    "            Sub_Index += 1\n",
    "            \n",
    "    Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block uses the lists to populate the DataFrame columns for Company Name, S&P Symbol, and S&P Weight\n",
    "\n",
    "S_and_P_DF['Company'] = List_of_Companies\n",
    "S_and_P_DF['Symbol'] = List_of_Symbols\n",
    "S_and_P_DF['Weight'] = List_of_Weights\n",
    "\n",
    "#S_and_P_DF # To view this DataFrame remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a list of Yahoo Finance URLs that can be used for company-specific API calls\n",
    "# The data on the Yahoo site contains a historical record of price changes, not just the most recent close price\n",
    "# Constructing each company's URL will require inserting the company's ticker symbol at the correct point\n",
    "\n",
    "List_of_Yahoo_Finance_History_URLs = []\n",
    "\n",
    "for symbol in List_of_Symbols:\n",
    "    \n",
    "    Historical_Data_URL = 'https://finance.yahoo.com/quote/' + str(symbol) + '/history?p=' + str(symbol)\n",
    "    List_of_Yahoo_Finance_History_URLs.append(Historical_Data_URL)\n",
    "    \n",
    "# Two listings (Berkshire and Brown Forman) have non-matching ticker symbols between the list and Yahoo Finance\n",
    "# The code below corrects them by replacing a \".\" with a \"-\" in the symbol part of the URL\n",
    "\n",
    "Berkshire_Index = (List_of_Yahoo_Finance_History_URLs.index('https://finance.yahoo.com/quote/BRK.B/history?p=BRK.B'))\n",
    "Brown_Forman_Index = (List_of_Yahoo_Finance_History_URLs.index('https://finance.yahoo.com/quote/BF.B/history?p=BF.B'))\n",
    "\n",
    "List_of_Yahoo_Finance_History_URLs[Berkshire_Index] = 'https://finance.yahoo.com/quote/BRK-B/history?p=BRK-B'\n",
    "List_of_Yahoo_Finance_History_URLs[Brown_Forman_Index] = 'https://finance.yahoo.com/quote/BF-B/history?p=BF-B'\n",
    "\n",
    "#List_of_Yahoo_Finance_History_URLs # To view the list of URLs remove the left most hashtag from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block scapes closing price and adjusted closing price data from a selected historical date\n",
    "# Note: The date I originally used ifor this part of the project was 'Sep 05, 2019'\n",
    "# There is a limit to how far back one can scrape with this site and it was Sept. 5 2019 at the time\n",
    "# Further Note: Two companies (Otis Worldwide (OTIS) and Carrier Global (CARR)) were later added to the S&P\n",
    "# The Yahoo Finance pages for these 2 new companies only have stock price data going back to March 19, 2020\n",
    "\n",
    "Date_To_Scrape_From = \"Mar 19, 2020\" \n",
    "# In this example, March 19, 2020 was used so every entry (including recent additions) will have a closing price\n",
    "# Alternatively, a user can choose to go further back and assign NaN values to the few that don't have prices\n",
    "# See the code for that below\n",
    "\n",
    "List_of_Historic_Closing_Prices = []\n",
    "List_of_Historic_Adjusted_Closing_Prices = []\n",
    "\n",
    "URLs_Not_Appended = [] # List to check if a company failed to have data scraped (should be empty)\n",
    "URLs_Multiple_Appended = [] # List to check if a company had multiple rows of data scraped (should be empty)\n",
    "\n",
    "URL_Index = 0\n",
    "\n",
    "Check_Interval = 20 # If you want to monitor the code blocks progress at regular intervals, you can adjust this\n",
    "# This will determine the frequency of progress updates (eg: \"20 complete\", \"40 complete\", etc if interval = 20)\n",
    "\n",
    "while(URL_Index < len(List_of_Yahoo_Finance_History_URLs)):\n",
    "\n",
    "    URL = List_of_Yahoo_Finance_History_URLs[URL_Index]\n",
    "    \n",
    "    Added = False\n",
    "    Appends = 0\n",
    "    Attempted_APIs = 0\n",
    "\n",
    "    while((Attempted_APIs < 5) and (Added == False)):\n",
    "        \n",
    "        response = requests.get(URL)\n",
    "        if(response.status_code != 200):\n",
    "            print(URL_Index, URL, \"Failed API\", response.status_code)    \n",
    "        Historic_Data_Text = response.text\n",
    "        Historic_Data_List = list((Historic_Data_Text).split(\"<tr\"))\n",
    "    \n",
    "        for Row in Historic_Data_List:\n",
    "            if(Date_To_Scrape_From in Row):\n",
    "                if(\"Dividend\" in Row):\n",
    "                    pass\n",
    "                else:\n",
    "                    Span_List = list(Row.split(\"<span\"))\n",
    "                    Append_List = []\n",
    "                    for Span in Span_List:\n",
    "                        Index = 0\n",
    "                        while(Span[Index] != \">\"):\n",
    "                            Index += 1\n",
    "                        Index += 1\n",
    "                        End_Index = Index\n",
    "                        while(Span[End_Index] != \"<\"):\n",
    "                            End_Index += 1\n",
    "                        Append_List.append(Span[Index:End_Index])\n",
    "                    if(len(Append_List) > 6):\n",
    "                        List_of_Historic_Closing_Prices.append(Append_List[5])\n",
    "                        List_of_Historic_Adjusted_Closing_Prices.append(Append_List[6])\n",
    "                        Added = True\n",
    "                        Appends += 1\n",
    "                    else:\n",
    "                        Added = True\n",
    "                        List_of_Historic_Closing_Prices.append(np.nan)\n",
    "                        List_of_Historic_Adjusted_Closing_Prices.append(np.nan)\n",
    "\n",
    "        if(Added == False):\n",
    "            print(URL_Index, URL, \"Not Added\")\n",
    "            Attempted_APIs += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        if(Appends == 0):\n",
    "            URLs_Not_Appended.append(URL)\n",
    "\n",
    "        if(Appends > 1):\n",
    "            URLs_Multiple_Appended.append(URL)\n",
    "            \n",
    "    if(Added == False):\n",
    "        List_of_Historic_Closing_Prices.append(np.nan)\n",
    "        List_of_Historic_Adjusted_Closing_Prices.append(np.nan)\n",
    "        \n",
    "    URL_Index += 1\n",
    "    \n",
    "    if((URL_Index % Check_Interval) == 0): # Un-comment-ify this if-block to get progress updates\n",
    "        print(URL_Index, \"complete\")\n",
    "        \n",
    "print(len(List_of_Historic_Closing_Prices)) # To verify all companies had an entry added, check the list's length\n",
    "# There should be 505 entries on this list (a few companies are doubly listed on the S&P 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the code below to check if any URL API calls failed to get closing prices for the date\n",
    "\n",
    "print(URLs_Not_Appended) # No closing price (np.nan appended for that entry)\n",
    "print(URLs_Multiple_Appended) # Multiple closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block updates the DataFrame with the closing prices from the relevant dates\n",
    "\n",
    "S_and_P_DF['Mar 19, 2020 Close'] = List_of_Historic_Closing_Prices\n",
    "S_and_P_DF['Mar 19, 2020 Adjusted Close'] = List_of_Historic_Adjusted_Closing_Prices\n",
    "S_and_P_DF['Most Recent Closing Price'] = List_of_Closings\n",
    "\n",
    "#display(S_and_P_DF) # To view the updated dataframe remove the hashtag in front of the display command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines the NoCommas(string) function\n",
    "# This function helps convert string representations of prices/finance into float values if the string has commas\n",
    "\n",
    "def NoCommas(string):\n",
    "    \n",
    "    Output = \"\"\n",
    "    String_Index = 0\n",
    "    \n",
    "    while(String_Index < len(string)):\n",
    "        if(string[String_Index] == \",\"):\n",
    "            pass\n",
    "        else:\n",
    "            Output = Output + str(string[String_Index])\n",
    "            \n",
    "        String_Index += 1\n",
    "        \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block converts string variables to float variables in the DataFrame\n",
    "\n",
    "Index = 0\n",
    "\n",
    "while(Index < len(S_and_P_DF)):\n",
    "    \n",
    "    if(type(S_and_P_DF['Most Recent Closing Price'][Index]) == str):\n",
    "        S_and_P_DF['Most Recent Closing Price'][Index] = NoCommas(S_and_P_DF['Most Recent Closing Price'][Index])\n",
    "        S_and_P_DF['Most Recent Closing Price'][Index] = float(S_and_P_DF['Most Recent Closing Price'][Index])\n",
    "    \n",
    "    if(type(S_and_P_DF['Mar 19, 2020 Close'][Index]) == str):\n",
    "        S_and_P_DF['Mar 19, 2020 Close'][Index] = NoCommas(S_and_P_DF['Mar 19, 2020 Close'][Index])\n",
    "        S_and_P_DF['Mar 19, 2020 Close'][Index] = float(S_and_P_DF['Mar 19, 2020 Close'][Index])\n",
    "    \n",
    "    if(type(S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index]) == str):\n",
    "        S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index] = NoCommas(S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index])\n",
    "        S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index] = float(S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index])\n",
    "    \n",
    "    Index += 1\n",
    "    \n",
    "#display(S_and_P_DF) # To view the updated dataframe remove the hashtag in front of the display command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block calculates the percentage increase from the scraped historical date to the recent close\n",
    "# The precent is calculated as a decimal (eg 20% == 0.20) but you can multiply by 100% to get normal percentage\n",
    "# Stocks that declined in price during that time have a negative percent increase\n",
    "\n",
    "Index = 0\n",
    "\n",
    "List_of_Increases = []\n",
    "List_of_Percent_Increases = []\n",
    "\n",
    "while(Index < len(S_and_P_DF)):\n",
    "    Inc = (float(S_and_P_DF['Most Recent Closing Price'][Index]) - float(S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index]))\n",
    "    Per = Inc/float(S_and_P_DF['Mar 19, 2020 Adjusted Close'][Index])\n",
    "    \n",
    "    List_of_Increases.append(Inc)\n",
    "    List_of_Percent_Increases.append(Per)\n",
    "    \n",
    "    Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block adds the increase amount and percentage to the DataFrame\n",
    "\n",
    "S_and_P_DF['Gross Increase'] = List_of_Increases\n",
    "S_and_P_DF['Percent Increase'] = List_of_Percent_Increases\n",
    "\n",
    "#display(S_and_P_DF) # To view the updated DataFrame remove the hashtag in front of the display command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block identifies creates a new DataFrame from a CSV file with data on each company's industry\n",
    "# You must use the 'constituents_csv' CSV file for this block along with your own file path\n",
    "\n",
    "Industry_DF = pd.read_csv('/Your_File_Path/constituents_csv.csv') # Replace with your file path\n",
    "\n",
    "#display(Industry_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block merges the two DataFrames by joining on symbol\n",
    "\n",
    "S_and_P_With_Industry_DF = pd.DataFrame(S_and_P_DF.join(Industry_DF.set_index('Symbol'), on='Symbol'))\n",
    "\n",
    "display(S_and_P_With_Industry_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block fills in missing industry sector values with \"Other\"\n",
    "\n",
    "S_and_P_With_Industry_DF['Sector'] = S_and_P_With_Industry_DF.Sector.fillna('Other')\n",
    "    \n",
    "#display(S_and_P_With_Industry_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block checks for all unique industries values that populate the Sector Field\n",
    "\n",
    "S_and_P_With_Industry_DF['Sector'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to create one-hot encoded dummy variables, this code block creates a list for each industry\n",
    "# Each industry list will have an entry for each company with either a \"1\" (if in that sector) or a \"0\"\n",
    "\n",
    "Uniques = ['Information Technology', 'Consumer Discretionary', 'Financials', 'Health Care', 'Consumer Staples',\n",
    "           'Energy', 'Telecommunication Services', 'Industrials', 'Utilities','Real Estate', 'Materials']\n",
    "\n",
    "Information_Tech_List = []\n",
    "Consumer_Discretionary_List = []\n",
    "Customer_Staples_List = []\n",
    "Financials_List = []\n",
    "Healthcare_List = []\n",
    "Energy_List = []\n",
    "Telecom_List = []\n",
    "Industries_List = []\n",
    "Utilities_List = []\n",
    "Real_Estate_List = []\n",
    "Materials_List = []\n",
    "\n",
    "List_of_Lists = [Information_Tech_List, Consumer_Discretionary_List, Financials_List, Healthcare_List,\n",
    "                 Customer_Staples_List, Energy_List, Telecom_List, Industries_List, Utilities_List,\n",
    "                 Real_Estate_List, Materials_List]\n",
    "\n",
    "Index = 0\n",
    "\n",
    "while(Index < len(S_and_P_With_Industry_DF)):\n",
    "    List_Index = 0\n",
    "    Industry_Found = False\n",
    "    while((Industry_Found == False) and (List_Index < len(Uniques))):\n",
    "        if(S_and_P_With_Industry_DF['Sector'][Index] == Uniques[List_Index]):\n",
    "            List_of_Lists[List_Index].append(1)\n",
    "            Industry_Found = True\n",
    "        else:\n",
    "            List_of_Lists[List_Index].append(0)\n",
    "        List_Index += 1\n",
    "    while(List_Index < len(Uniques)):\n",
    "        List_of_Lists[List_Index].append(0)\n",
    "        List_Index += 1\n",
    "    Index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates and populates columns for the one-hot encoded sector variables\n",
    "\n",
    "S_and_P_With_Industry_DF['Information Technology'] = Information_Tech_List\n",
    "S_and_P_With_Industry_DF['Consumer Discretionary'] = Consumer_Discretionary_List\n",
    "S_and_P_With_Industry_DF['Financials'] = Financials_List\n",
    "S_and_P_With_Industry_DF['Health Care'] = Healthcare_List\n",
    "S_and_P_With_Industry_DF['Consumer Staples'] = Customer_Staples_List\n",
    "S_and_P_With_Industry_DF['Energy'] = Energy_List\n",
    "S_and_P_With_Industry_DF['Telecommunication Services'] = Telecom_List\n",
    "S_and_P_With_Industry_DF['Industrials'] = Industries_List\n",
    "S_and_P_With_Industry_DF['Utilities'] = Utilities_List\n",
    "S_and_P_With_Industry_DF['Real Estate'] = Real_Estate_List\n",
    "S_and_P_With_Industry_DF['Materials'] = Materials_List\n",
    "\n",
    "display(S_and_P_With_Industry_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block scrapes financial data (Total Revenue, Gross Profit, Operating Expense, etc) for each company\n",
    "# This block takes a while to run due to a need for a delay between API calls\n",
    "# To keep track of progress, remove the \"#\"s from in front of the print statements\n",
    "\n",
    "symbol = [] # Company symbol\n",
    "name = [] # Name of field (\"Total Revenue\", \"Gross Profit\", etc)\n",
    "value = [] # Value in field\n",
    "\n",
    "for i, s in enumerate(List_of_Symbols):\n",
    "    \n",
    "    Financials_URL = 'https://finance.yahoo.com/quote/' + s + '/financials?p=' + s\n",
    "    API_Attempts = 0\n",
    "    Done_With_Company = False\n",
    "    \n",
    "    while((API_Attempts < 5) and (Done_With_Company == False)):\n",
    "        \n",
    "        response = requests.get(Financials_URL)\n",
    "        \n",
    "        if(response.status_code == 200):\n",
    "            soup = BeautifulSoup(response.text, \"html5lib\")\n",
    "            Fin_Elements = soup.find(class_='D(tbrg)')\n",
    "            print(\" \")\n",
    "            print(\"--------------------------------------------------------------------------------\")\n",
    "            if Fin_Elements:\n",
    "                print(\" \")\n",
    "                print(Financials_URL, \"Success\")\n",
    "                print(\" \")\n",
    "                for rows in Fin_Elements.find_all(class_='D(tbr)'):\n",
    "                    elems = rows.find_all(class_='D(tbc)')\n",
    "                    symbol.append(s)\n",
    "                    name.append(elems[0].text)\n",
    "                    value.append(elems[1].text)\n",
    "            else:\n",
    "                print(\" \")\n",
    "                print(Financials_URL, \"<--------- API Success, Scraping Fail:\", response.status_code)    \n",
    "                print(\" \")\n",
    "            Done_With_Company = True\n",
    "        else:\n",
    "            API_Attempts += 1\n",
    "            print(\" \")\n",
    "            print(\"--------------------------------------------------------------------------------\")\n",
    "            print(\" \")\n",
    "            print(Financials_URL, \"<--- Failed API Attempt:\", API_Attempts, response.status_code)    \n",
    "            print(\" \") \n",
    "            time.sleep(3 ** (API_Attempts))\n",
    "            \n",
    "    if(Done_With_Company == False):\n",
    "        print(\" \")\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\" \")\n",
    "        print(Financials_URL, \"<-------- Failed  All API Attempts:\", response.status_code)   \n",
    "        print(\" \")\n",
    "        \n",
    "    time.sleep(2) # This line imposes a delay between API calls to avoid crashing the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a DataFrams with company financil data, data category, and company symbol\n",
    "\n",
    "Financials_DF = pd.DataFrame()\n",
    "Financials_DF['Symbol'] = symbol\n",
    "Financials_DF['Name'] = name\n",
    "Financials_DF['Value'] = value\n",
    "\n",
    "#display(Financials_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block creates a new DataFrame with a column for each type of financial data\n",
    "# The values in the 'name' column of the previous DataFrame become column names for the new DataFrame\n",
    "\n",
    "Financials_Unstacked_DF = Financials_DF.groupby(['Symbol','Name']).max()\n",
    "Financials_Unstacked_DF = Financials_Unstacked_DF.unstack(1)\n",
    "Financials_Unstacked_DF.columns = Financials_Unstacked_DF.columns.droplevel()\n",
    "Financials_Unstacked_DF['Symbol'] = Financials_Unstacked_DF.index\n",
    "\n",
    "#display(Financials_Unstacked_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block adjusts the symbol for Berkshire Hathaway\n",
    "\n",
    "Financials_Unstacked_DF['Symbol']['BRK.B'] = 'BRK-B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block allows the user to view which columns have been successfully populated and for how many entries\n",
    "\n",
    "Financials_Unstacked_DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block merges the Finacials dataframe with the existing S&P dataframe to create the full data set\n",
    "\n",
    "Full_S_and_P_DF = pd.DataFrame(S_and_P_With_Industry_DF.join(Financials_Unstacked_DF.set_index('Symbol'), on='Symbol'))\n",
    "\n",
    "display(Full_S_and_P_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block saves the DataFrame in the current folder as a pickle file \n",
    "\n",
    "with open('Full_S_and_P_DF.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(Full_S_and_P_DF, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
